import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted
from typing import Any

class WoeEncoder(BaseEstimator, TransformerMixin):
    """
    Custom Scikit-Learn transformer for Weight of Evidence (WoE) encoding.
    """

    def __init__(self, columns: list[str] | None = None, regularization: float = 1.0):
        self.columns = columns
        self.regularization = regularization
        self.mapping_: dict[str, dict[str | float, float]] = {}
        self.iv_: dict[str, float] = {}

    def _ensure_dataframe(self, X: Any) -> pd.DataFrame:
        """Ensures input is a DataFrame, even when it is a NumPy array."""
        if isinstance(X, pd.DataFrame):
            return X
        if isinstance(X, np.ndarray):
            if self.columns is not None:
                return pd.DataFrame(X, columns=self.columns)
            return pd.DataFrame(X, columns=[f'col_{i}' for i in range(X.shape[1])])
        raise TypeError(f"Unsupported input type: {type(X)}")

    def fit(self, X: pd.DataFrame, y: pd.Series):
        """
        Computes WoE values for each specified column.
        """
        X = self._ensure_dataframe(X)
        
        if self.columns is None:
             self.columns = X.columns.tolist()

        total_bads = y.sum()
        total_goods = y.count() - total_bads
        
        if total_bads == 0 or total_goods == 0:
            raise ValueError("Target 'y' must contain both classes 0 and 1.")

        for col in self.columns:
            df_calc = pd.DataFrame({'feature': X[col], 'target': y})
            
            stats = df_calc.groupby('feature', dropna=False)['target'].agg(['count', 'sum'])
            stats.columns = ['total', 'bads']
            stats['goods'] = stats['total'] - stats['bads']
            
            dist_goods = (stats['goods'] + self.regularization) / (total_goods + 2 * self.regularization)
            dist_bads = (stats['bads'] + self.regularization) / (total_bads + 2 * self.regularization)
            
            woe_values = np.log(dist_goods / dist_bads)
            
            iv_contribution = (dist_goods - dist_bads) * woe_values
            self.iv_[col] = float(iv_contribution.sum())
            
            # --- DICTIONARY NORMALIZATION FIX ---
            d = woe_values.to_dict()
            
            # Look for keys representing NaN (np.nan, float('nan'), None...)
            nan_keys = [k for k in d.keys() if pd.isna(k)]
            
            if nan_keys:
                # If a NaN key already exists (e.g., generated by groupby), normalize the key to np.nan
                val = d[nan_keys[0]]
                # Remove all NaN variants to avoid duplicates
                for k in nan_keys:
                    del d[k]
                # Insert a clean NaN key
                d[np.nan] = val
            else:
                # If no NaN category appeared during training, set default WoE value to 0.0
                d[np.nan] = 0.0
            
            self.mapping_[col] = d

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Replaces categories with their corresponding WoE values.
        """
        check_is_fitted(self, 'mapping_')
        X_out = self._ensure_dataframe(X)

        if self.columns is None:
             return X_out

        for col in self.columns:
            if col in X_out.columns:
                mapped_series = X_out[col].map(self.mapping_[col])
                X_out[col] = mapped_series.fillna(0.0).astype(float)
                
        return X_out
